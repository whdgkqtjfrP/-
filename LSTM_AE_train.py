# -*- coding: utf-8 -*-
"""LSTM_AE_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oPoHkiqOWFyKh3u3HnxIBiH9unIAG8j0
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping

# LSTM-AE 모델을 위한 데이터
X = X_lstm_AE

# 데이터 분할 (훈련 80%, 테스트 20%)
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)

# LSTM 모델의 입력 데이터에 맞는 float32로 변환
X_train = X_train.astype(np.float32)
X_test = X_test.astype(np.float32)

# LSTM-Autoencoder 모델 설계
input_dim = X.shape[2]  # Feature 개수 (n_features)
timesteps = X.shape[1]  # timesteps 개수

# 인코더 (입력 → 잠재 공간)
input_layer = Input(shape=(timesteps, input_dim))
encoded = LSTM(32, activation="tanh", return_sequences=True)(input_layer)  # 첫 번째 LSTM (시퀀스를 유지)
encoded = LSTM(16, activation="tanh", return_sequences=False)(encoded)  # 두 번째 LSTM (잠재 공간으로 압축)
encoded = RepeatVector(timesteps)(encoded)  # 시퀀스 길이 복원

# 디코더 (복원 과정)
decoded = LSTM(16, activation="tanh", return_sequences=True)(encoded)  # 첫 번째 LSTM (시퀀스 복원)
decoded = LSTM(32, activation="tanh", return_sequences=True)(decoded)  # 두 번째 LSTM (시퀀스 복원)
decoded = TimeDistributed(Dense(input_dim, activation="sigmoid"))(decoded)  # 원래 차원 복원

# 모델 정의
lstm_autoencoder = Model(input_layer, decoded)
lstm_autoencoder.compile(optimizer="adam", loss="mse")

# EarlyStopping을 설정하여 과적합 방지
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# 모델 학습
history = lstm_autoencoder.fit(X_train, X_train, epochs=20, batch_size=256, validation_data=(X_test, X_test), callbacks=[early_stopping])

# 학습 과정 시각화 (Loss)
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Train Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.xlim(left=0)  # x축 0부터 시작
plt.ylim(bottom=0)  # y축 0부터 시작
plt.legend()
plt.title("LSTM Autoencoder Training Loss")
plt.show()

# 모델 저장
lstm_autoencoder.save("lstm_autoencoder_model.h5")
print("모델이 'lstm_autoencoder_model.h5' 파일로 저장 완료")

#학습 데이터(X_train)로 재구성 오류(MSE) 계산
X_train_pred = lstm_autoencoder.predict(X_train)  # 모델이 재구성한 데이터
mse_loss_train = np.mean((X_train - X_train_pred) ** 2, axis=(1, 2))  # MSE 계산

#평균 + 표준편차 기반
mean_mse = np.mean(mse_loss_train)
std_mse = np.std(mse_loss_train)
threshold = mean_mse + 2.5 * std_mse

#Threshold 값 확인
print(f"설정된 이상 탐지 Threshold: {threshold}")

#재구성 오류 히스토그램 시각화
plt.figure(figsize=(8, 5))
plt.hist(mse_loss_train, bins=50, color="blue", alpha=0.7, label="Train MSE Loss")
plt.axvline(threshold, color="red", linestyle="dashed", linewidth=2, label="Threshold")
plt.xlabel("Reconstruction Error (MSE)")
plt.ylabel("Frequency")
plt.legend()
plt.title("MSE distrubution")
plt.show()



